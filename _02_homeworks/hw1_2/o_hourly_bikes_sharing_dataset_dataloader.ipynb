{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17520, 18]) !!!\n",
      "Train: 17376, Validation: 96, Test: 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "# 주피터 노트북에서는 __file__ 변수가 정의되지 않기에 직접 설정할 것임\n",
    "# BASE_PATH = str(Path(__file__).resolve().parent.parent.parent) # BASE_PATH: /Users/yhhan/git/link_dl\n",
    "\n",
    "\n",
    "# 본 과제 제출자는 현재 우분투 도커 환경에서 작업중이므로 다음과 같이 경로 설정\n",
    "BASE_PATH=\"/home/Deep-Learning-study\"\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# 자전거 대여 데이터 처리 클래스\n",
    "class HourlyBikesDataset(Dataset):\n",
    "  def __init__(self, X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "\n",
    "  # 데이터셋 크기 반환\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "\n",
    "  # 인덱스의 데이터 반환\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  # 데이터셋 정보 반환\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str\n",
    "\n",
    "\n",
    "# 데이터셋 처리 및 분할 함수\n",
    "def get_hourly_bikes_data(sequence_size=24, validation_size=96, test_size=24, y_normalizer=100):\n",
    "  bikes_path = os.path.join(BASE_PATH, \"_00_data\", \"e_time-series-bike-sharing-dataset\", \"hour-fixed.csv\")\n",
    "\n",
    "  bikes_numpy = np.loadtxt(\n",
    "    fname=bikes_path, dtype=np.float32, delimiter=\",\", skiprows=1,\n",
    "    converters={\n",
    "      1: lambda x: float(x[8:10])  # 2011-01-07 --> 07 --> 7\n",
    "    }\n",
    "  )\n",
    "\n",
    "  # 넘파이 배열을 텐서로 변환\n",
    "  bikes_data = torch.from_numpy(bikes_numpy).to(torch.float) # >>> torch.Size([17520, 17])\n",
    "  bikes_target = bikes_data[:, -1].unsqueeze(dim=-1)  # 'cnt'\n",
    "  # 마지막 열 ('cnt')을 제외한 데이터 사용\n",
    "  bikes_data = bikes_data[:, :-1]  # >>> torch.Size([17520, 16])\n",
    "\n",
    "\n",
    "  # 원핫 코딩을 위한 아이덴티티 메트릭스 생성\n",
    "  eye_matrix = torch.eye(4)\n",
    "\n",
    "# 원핫 인코딩\n",
    "  data_torch_list = []\n",
    "  for idx in range(bikes_data.shape[0]):  # range(730)\n",
    "    hour_data = bikes_data[idx]  # day.shape: [24, 17]\n",
    "    weather_onehot = eye_matrix[hour_data[9].long() - 1]\n",
    "    concat_data_torch = torch.cat(tensors=(hour_data, weather_onehot), dim=-1)  # day_torch.shape: [24, 21]\n",
    "    data_torch_list.append(concat_data_torch)\n",
    "\n",
    "  # 리스트를 텐서로 변환\n",
    "  bikes_data = torch.stack(data_torch_list, dim=0)\n",
    "  bikes_data = torch.cat([bikes_data[:, 1:9], bikes_data[:, 10:]], dim=-1)\n",
    "  print(bikes_data.shape, \"!!!\")  # >>> torch.Size([17520, 18])\n",
    "\n",
    "  data_size = len(bikes_data) - sequence_size\n",
    "  train_size = data_size - (validation_size + test_size)\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "  row_cursor = 0\n",
    "\n",
    "\n",
    "  # 학습 데이터 생성\n",
    "  X_train_list = []\n",
    "  y_train_regression_list = []\n",
    "  for idx in range(0, train_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_train_list.append(sequence_data)\n",
    "    y_train_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "  y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  m = X_train.mean(dim=0, keepdim=True)\n",
    "  s = X_train.std(dim=0, keepdim=True)\n",
    "  X_train = (X_train - m) / s\n",
    "\n",
    "  #################################################################################################\n",
    "\n",
    "\n",
    "  # 검증 데이터 생성\n",
    "  X_validation_list = []\n",
    "  y_validation_regression_list = []\n",
    "  for idx in range(row_cursor, row_cursor + validation_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_validation_list.append(sequence_data)\n",
    "    y_validation_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "  y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  X_validation -= m\n",
    "  X_validation /= s\n",
    "  #################################################################################################\n",
    "\n",
    "  # 테스트 데이터 생성\n",
    "  X_test_list = []\n",
    "  y_test_regression_list = []\n",
    "  for idx in range(row_cursor, row_cursor + test_size):\n",
    "    sequence_data = bikes_data[idx: idx + sequence_size]\n",
    "    sequence_target = bikes_target[idx + sequence_size - 1]\n",
    "    X_test_list.append(sequence_data)\n",
    "    y_test_regression_list.append(sequence_target)\n",
    "    row_cursor += 1\n",
    "\n",
    "  X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "  y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "\n",
    "  X_test -= m\n",
    "  X_test /= s\n",
    "\n",
    "  return (\n",
    "    X_train, X_validation, X_test,\n",
    "    y_train_regression, y_validation_regression, y_test_regression\n",
    "  )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  X_train, X_validation, X_test, y_train, y_validation, y_test = get_hourly_bikes_data(\n",
    "    sequence_size=24, validation_size=96, test_size=24, y_normalizer=100\n",
    "  )\n",
    "\n",
    "  print(\"Train: {0}, Validation: {1}, Test: {2}\".format(len(X_train), len(X_validation), len(X_test)))\n",
    "\n",
    "  train_hourly_bikes_dataset = HourlyBikesDataset(X=X_train, y=y_train)\n",
    "  validation_hourly_bikes_dataset = HourlyBikesDataset(X=X_validation, y=y_validation)\n",
    "  test_houly_bikes_dataset = HourlyBikesDataset(X=X_test, y=y_test)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_hourly_bikes_dataset, batch_size=32, shuffle=True, drop_last=True\n",
    "  )\n",
    "\n",
    "  # for idx, batch in enumerate(train_data_loader):\n",
    "  #   input, target = batch\n",
    "  #   print(\"{0} - {1}: {2}, {3}\".format(idx, input.shape, target.shape, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
