{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    # X랑 y 받아서 Tensor로 변환\n",
    "    def __init__(self, X, y):\n",
    "        # FloatTensor로 바꿈\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    # 데이터셋 크기 반환\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # 인덱스 받아서 데이터 하나씩 출력\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return {'input': feature, 'target': target}\n",
    "    \n",
    "    # 데이터셋 정보 문자열로 반환 \n",
    "    def __str__(self):\n",
    "        str = \"data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(len(self.X), self.X.shape, self.y.shape)\n",
    "        return str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicTestDataset(Dataset):\n",
    "    # Tensor로 변환\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "    \n",
    "    # 데이터 개수 반환\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # 인덱스 받아서 데이터 출력\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature}\n",
    "    \n",
    "    # 데이터셋 정보 문자열로 반환\n",
    "    def __str__(self):\n",
    "        str = \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "            len(self.X), self.X.shape\n",
    "        )\n",
    "        return str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset_1(all_df):\n",
    "    # Pclass별 Fare 평균값을 사용하여 Fare 결측치 메우기\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    # name을 세 개의 컬럼으로 분리하여 다시 all_df에 합침\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    # honorific별 Age 평균값을 사용하여 Age 결측치 메우기\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\", ]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    # 가족수(family_num) 컬럼 새롭게 추가\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "\n",
    "    # 혼자탑승(alone) 컬럼 새롭게 추가\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "\n",
    "    # 학습에 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    # honorific 값 개수 줄이기\n",
    "    all_df.loc[\n",
    "    ~(\n",
    "            (all_df[\"honorific\"] == \"Mr\") |\n",
    "            (all_df[\"honorific\"] == \"Miss\") |\n",
    "            (all_df[\"honorific\"] == \"Mrs\") |\n",
    "            (all_df[\"honorific\"] == \"Master\")\n",
    "    ),\n",
    "    \"honorific\"\n",
    "    ] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "    return all_df\n",
    "\n",
    "\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    # 카테고리 변수를 LabelEncoder를 사용하여 수치값으로 변경하기\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "          le = le.fit(all_df[category_feature])\n",
    "          all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_dataset():\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    #print(dataset)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "    #print(test_dataset)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# 모델 정의 및 최적 activation 찾기\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output, activation_fn=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, 30),\n",
    "            activation_fn(),  # activation_fn을 호출하여 인스턴스를 생성\n",
    "            nn.Linear(30, 30),\n",
    "            activation_fn(),  # activation_fn을 호출하여 인스턴스를 생성\n",
    "            nn.Linear(30, n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수\n",
    "def test(test_data_loader, model):\n",
    "    print(\"[TEST]\")\n",
    "    batch = next(iter(test_data_loader))\n",
    "    print(\"{0}\".format(batch['input'].shape))\n",
    "    output_batch = model(batch['input'])\n",
    "    prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "    for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "        print(idx, prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 생성 함수\n",
    "def create_submission(test_data_loader, model, submission_file='/home/Deep-Learning-study/_02_homeworks/hw2/submission.csv'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_data_loader:\n",
    "            outputs = model(batch['input'])\n",
    "            prediction_batch = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(prediction_batch.cpu().numpy())\n",
    "\n",
    "    # 지정된 경로에 submission.csv 파일 생성\n",
    "    submission_df = pd.DataFrame({'PassengerId': range(892, 892 + len(predictions)), 'Survived': predictions})\n",
    "    \n",
    "    # 디렉토리 확인 및 생성\n",
    "    directory = os.path.dirname(submission_file)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    submission_df.to_csv(submission_file, index=False)\n",
    "    print(f\"Submission file '{submission_file}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 함수\n",
    "def train_and_validate(train_data_loader, validation_data_loader, model, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for batch in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch['input'])\n",
    "            loss = criterion(outputs, batch['target'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_data_loader)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in validation_data_loader:\n",
    "                outputs = model(batch['input'])\n",
    "                loss = criterion(outputs, batch['target'])\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        validation_loss /= len(validation_data_loader)\n",
    "\n",
    "        # Log losses to wandb\n",
    "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": train_loss, \"validation_loss\": validation_loss})\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/Deep-Learning-study/_02_homeworks/hw2/wandb/run-20241022_150222-xz49bf63</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch/runs/xz49bf63' target=\"_blank\">ruby-puddle-17</a></strong> to <a href='https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch' target=\"_blank\">https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch/runs/xz49bf63' target=\"_blank\">https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch/runs/xz49bf63</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1011/1050572811.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_1011/1050572811.py:59: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: 713, validation_dataset.shape: 178, test_dataset: 418\n",
      "Epoch [1/10], Train Loss: 0.8741, Validation Loss: 0.6960\n",
      "Epoch [2/10], Train Loss: 0.5993, Validation Loss: 0.6275\n",
      "Epoch [3/10], Train Loss: 0.5847, Validation Loss: 0.6462\n",
      "Epoch [4/10], Train Loss: 0.5714, Validation Loss: 0.6157\n",
      "Epoch [5/10], Train Loss: 0.5542, Validation Loss: 0.6489\n",
      "Epoch [6/10], Train Loss: 0.5604, Validation Loss: 0.5910\n",
      "Epoch [7/10], Train Loss: 0.5524, Validation Loss: 0.5934\n",
      "Epoch [8/10], Train Loss: 0.5442, Validation Loss: 0.5866\n",
      "Epoch [9/10], Train Loss: 0.5298, Validation Loss: 0.5964\n",
      "Epoch [10/10], Train Loss: 0.5308, Validation Loss: 0.5862\n",
      "Submission file '/home/Deep-Learning-study/_02_homeworks/hw2/submission.csv' created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▂▂▂▁▂▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▄▅▃▅▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.53079</td></tr><tr><td>validation_loss</td><td>0.58621</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-puddle-17</strong> at: <a href='https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch/runs/xz49bf63' target=\"_blank\">https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch/runs/xz49bf63</a><br/> View project at: <a href='https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch' target=\"_blank\">https://wandb.ai/jaeminyu2356-korea-university-of-technology-and-education/titanic-pytorch</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241022_150222-xz49bf63/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 메인 함수\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.init(project=\"titanic-pytorch\")\n",
    "    train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "    print(\"train_dataset: {0}, validation_dataset.shape: {1}, test_dataset: {2}\".format(\n",
    "        len(train_dataset), len(validation_dataset), len(test_dataset)\n",
    "    ))\n",
    "\n",
    "    # 데이터 로더 설정\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=False)\n",
    "    test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    # 모델 및 손실 함수, 최적화기 초기화\n",
    "    my_model = MyModel(n_input=11, n_output=2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(my_model.parameters(), lr=0.001)\n",
    "\n",
    "    # 학습 및 검증\n",
    "    num_epochs = 10\n",
    "    train_and_validate(train_data_loader, validation_data_loader, my_model, criterion, optimizer, num_epochs)\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    create_submission(test_data_loader, my_model)\n",
    "\n",
    "    # wandb 종료\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 총 학습의 전 과정을 대략적으로 경험할 수 있던 시간이어서 유익한 시간이었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
