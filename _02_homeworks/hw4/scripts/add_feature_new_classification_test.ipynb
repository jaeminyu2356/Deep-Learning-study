{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 본 과제 제출자는 현재 우분투 도커 환경에서 작업중이므로 다음과 같이 경로 설정\n",
    "BASE_PATH=\"/home/Deep-Learning-study\"\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "CURRENT_FILE_PATH = os.getcwd()\n",
    "CHECKPOINT_FILE_PATH = os.path.join(CURRENT_FILE_PATH, \"checkpoints\")\n",
    "\n",
    "if not os.path.isdir(CHECKPOINT_FILE_PATH):\n",
    "  os.makedirs(os.path.join(CURRENT_FILE_PATH, \"checkpoints\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _01_code._15_lstm_and_its_application.f_arg_parser import get_parser\n",
    "#from _01_code._15_lstm_and_its_application.g_crypto_currency_regression_train_lstm import get_btc_krw_data\n",
    "#from _01_code._15_lstm_and_its_application.i_crypto_currency_classification_train_lstm import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoCurrencyDataset(Dataset):\n",
    "  def __init__(self, X, y, is_regression=True):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "    assert len(self.X) == len(self.y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    X = self.X[idx]\n",
    "    y = self.y[idx]\n",
    "    return X, y\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_cryptocurrency_data(\n",
    "    sequence_size=10, validation_size=100, test_size=10, \n",
    "    target_column='Close', y_normalizer=1.0e7, \n",
    "    is_regression=True, use_next_open=True\n",
    "):\n",
    "    btc_krw_path = os.path.join(BASE_PATH, \"_00_data\", \"k_cryptocurrency\", \"BTC_KRW.csv\")\n",
    "    df = pd.read_csv(btc_krw_path)\n",
    "    row_size = len(df)\n",
    "    date_list = df['Date']\n",
    "\n",
    "    # Next_Open 컬럼 추가\n",
    "    if use_next_open:\n",
    "        df['Next_Open'] = df['Open'].shift(-1)\n",
    "        \n",
    "    df = df.drop(columns=['Date'])\n",
    "    \n",
    "    # Next_Open이 NaN인 마지막 행 제거\n",
    "    if use_next_open:\n",
    "        df = df.dropna()\n",
    "        row_size = len(df)\n",
    "\n",
    "    data_size = row_size - sequence_size\n",
    "    train_size = data_size - (validation_size + test_size)\n",
    "    #################################################################################################\n",
    "\n",
    "    row_cursor = 0\n",
    "\n",
    "    X_train_list = []\n",
    "    y_train_regression_list = []\n",
    "    y_train_classification_list = []\n",
    "    y_train_date = []\n",
    "    for idx in range(0, train_size):\n",
    "        sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 6)\n",
    "        X_train_list.append(torch.from_numpy(sequence_data))\n",
    "        y_train_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "        y_train_classification_list.append(\n",
    "            1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "        )\n",
    "        y_train_date.append(date_list[idx + sequence_size])\n",
    "        row_cursor += 1\n",
    "\n",
    "    X_train = torch.stack(X_train_list, dim=0).to(torch.float)\n",
    "    y_train_regression = torch.tensor(y_train_regression_list, dtype=torch.float32) / y_normalizer\n",
    "    y_train_classification = torch.tensor(y_train_classification_list, dtype=torch.int64)\n",
    "\n",
    "    m = X_train.mean(dim=0, keepdim=True)\n",
    "    s = X_train.std(dim=0, keepdim=True)\n",
    "    X_train = (X_train - m) / s\n",
    "\n",
    "    #################################################################################################\n",
    "\n",
    "    X_validation_list = []\n",
    "    y_validation_regression_list = []\n",
    "    y_validation_classification_list = []\n",
    "    y_validation_date = []\n",
    "    for idx in range(row_cursor, row_cursor + validation_size):\n",
    "        sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 6)\n",
    "        X_validation_list.append(torch.from_numpy(sequence_data))\n",
    "        y_validation_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "        y_validation_classification_list.append(\n",
    "            1 if df.iloc[idx + sequence_size][target_column] >= df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "        )\n",
    "        y_validation_date.append(date_list[idx + sequence_size])\n",
    "        row_cursor += 1\n",
    "\n",
    "    X_validation = torch.stack(X_validation_list, dim=0).to(torch.float)\n",
    "    y_validation_regression = torch.tensor(y_validation_regression_list, dtype=torch.float32) / y_normalizer\n",
    "    y_validation_classification = torch.tensor(y_validation_classification_list, dtype=torch.int64)\n",
    "\n",
    "    X_validation = (X_validation - m) / s\n",
    "    #################################################################################################\n",
    "\n",
    "    X_test_list = []\n",
    "    y_test_regression_list = []\n",
    "    y_test_classification_list = []\n",
    "    y_test_date = []\n",
    "    for idx in range(row_cursor, row_cursor + test_size):\n",
    "        sequence_data = df.iloc[idx: idx + sequence_size].values  # sequence_data.shape: (sequence_size, 6)\n",
    "        X_test_list.append(torch.from_numpy(sequence_data))\n",
    "        y_test_regression_list.append(df.iloc[idx + sequence_size][target_column])\n",
    "        y_test_classification_list.append(\n",
    "            1 if df.iloc[idx + sequence_size][target_column] > df.iloc[idx + sequence_size - 1][target_column] else 0\n",
    "        )\n",
    "        y_test_date.append(date_list[idx + sequence_size])\n",
    "        row_cursor += 1\n",
    "\n",
    "    X_test = torch.stack(X_test_list, dim=0).to(torch.float)\n",
    "    y_test_regression = torch.tensor(y_test_regression_list, dtype=torch.float32) / y_normalizer\n",
    "    y_test_classification = torch.tensor(y_test_classification_list, dtype=torch.int64)\n",
    "\n",
    "    X_test = (X_test - m) / s\n",
    "\n",
    "    if is_regression:\n",
    "        return (\n",
    "            X_train, X_validation, X_test,\n",
    "            y_train_regression, y_validation_regression, y_test_regression,\n",
    "            y_train_date, y_validation_date, y_test_date\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            X_train, X_validation, X_test,\n",
    "            y_train_classification, y_validation_classification, y_test_classification,\n",
    "            y_train_date, y_validation_date, y_test_date\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_btc_krw_data(sequence_size=21, validation_size=150, test_size=30, is_regression=True, use_next_open=True):\n",
    "    # use_next_open 파라미터 추가\n",
    "    X_train, X_validation, X_test, y_train, y_validation, y_test, y_train_date, y_validation_date, y_test_date \\\n",
    "        = get_cryptocurrency_data(\n",
    "            sequence_size=sequence_size,\n",
    "            validation_size=validation_size,\n",
    "            test_size=test_size,\n",
    "            target_column='Close',\n",
    "            y_normalizer=1.0e7,\n",
    "            is_regression=is_regression,\n",
    "            use_next_open=use_next_open  # Next_Open feature 사용 여부\n",
    "        )\n",
    "\n",
    "    # PyTorch Dataset 객체 생성\n",
    "    train_crypto_currency_dataset = CryptoCurrencyDataset(X=X_train, y=y_train)\n",
    "    validation_crypto_currency_dataset = CryptoCurrencyDataset(X=X_validation, y=y_validation)\n",
    "    test_crypto_currency_dataset = CryptoCurrencyDataset(X=X_test, y=y_test)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    train_data_loader = DataLoader(\n",
    "        dataset=train_crypto_currency_dataset,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    validation_data_loader = DataLoader(\n",
    "        dataset=validation_crypto_currency_dataset,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    test_data_loader = DataLoader(\n",
    "        dataset=test_crypto_currency_dataset,\n",
    "        batch_size=len(test_crypto_currency_dataset),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    return train_data_loader, validation_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def get_model():\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self, n_input=6, n_output=2):  # n_input을 6으로 변경 (Next_Open 포함)\n",
    "            super().__init__()\n",
    "            \n",
    "            #메인 LSTM 레이어\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=n_input,\n",
    "                hidden_size=1024,  # hidden size 증가\n",
    "                num_layers=3,      # 3개의 layer\n",
    "                dropout=0.1,       # dropout 추가\n",
    "                batch_first=True,\n",
    "                bidirectional=True # 양방향 LSTM\n",
    "            )\n",
    "            \n",
    "            # 분류를 위한 FC 레이어\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.LayerNorm(2048),  # bidirectional이므로 hidden_size * 2\n",
    "                nn.Linear(2048, 512),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                \n",
    "                nn.LayerNorm(512),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                \n",
    "                nn.LayerNorm(128),\n",
    "                nn.Linear(128, n_output),  # n_output=2 for binary classification\n",
    "            )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            self.lstm.flatten_parameters()  # CUDA 성능 최적화\n",
    "            x, _ = self.lstm(x)\n",
    "            x = x[:, -1, :]  # 마지막 시퀀스의 출력만 사용\n",
    "            x = self.fc_layers(x)\n",
    "            return x  # CrossEntropyLoss를 사용할 것이므로 softmax는 여기서 적용하지 않음\n",
    "\n",
    "    my_model = MyModel(n_input=6, n_output=2)\n",
    "    return my_model\n",
    "# Args 클래스도 classification task에 맞게 수정\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.wandb = True\n",
    "        self.batch_size = 32       # classification은 regression보다 큰 배치 사이즈가 효과적일 수 있음\n",
    "        self.epochs = 300\n",
    "        self.learning_rate = 1e-3  # classification은 보통 더 큰 학습률 사용\n",
    "        self.weight_decay = 1e-4\n",
    "        self.validation_intervals = 1\n",
    "        self.early_stop_patience = 30\n",
    "        self.early_stop_delta = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_model):\n",
    "    # 테스트용 데이터로더만 가져옴 (분류 태스크용)\n",
    "    _, _, test_data_loader = get_btc_krw_data(is_regression=False)\n",
    "\n",
    "    # 모델을 평가 모드로 설정 (dropout, batch norm 등이 평가 모드로 변경됨)\n",
    "    test_model.eval()\n",
    "\n",
    "    # 정확도 계산을 위한 변수 초기화\n",
    "    num_corrects_test = 0      # 정확히 예측한 샘플 수\n",
    "    num_tested_samples = 0     # 전체 테스트 샘플 수\n",
    "\n",
    "    print(\"[TEST DATA]\")\n",
    "    # gradient 계산 비활성화 (메모리 사용량 감소, 연산 속도 향상)\n",
    "    with torch.no_grad():\n",
    "        # 테스트 데이터의 배치를 하나씩 처리\n",
    "        for test_batch in test_data_loader:\n",
    "            # 입력 데이터와 정답 레이블을 배치에서 추출\n",
    "            input_test, target_test = test_batch\n",
    "\n",
    "            # 모델을 통해 예측값 계산\n",
    "            output_test = test_model(input_test)\n",
    "\n",
    "            # 가장 높은 확률을 가진 클래스를 예측값으로 선택\n",
    "            predicted_test = torch.argmax(output_test, dim=1)\n",
    "            # 정답과 예측이 일치하는 개수 누적\n",
    "            num_corrects_test += torch.sum(torch.eq(predicted_test, target_test))\n",
    "\n",
    "            # 처리된 샘플 수 누적\n",
    "            num_tested_samples += len(input_test)\n",
    "\n",
    "        # 전체 정확도 계산 (백분율)\n",
    "        test_accuracy = 100.0 * num_corrects_test / num_tested_samples\n",
    "\n",
    "        # 전체 테스트 정확도 출력\n",
    "        print(f\"TEST RESULTS: {test_accuracy:6.3f}%\")\n",
    "\n",
    "        # 각 샘플별 예측 결과 출력\n",
    "        for idx, (output, target) in enumerate(zip(output_test, target_test)):\n",
    "            # 결과 출력 포맷:\n",
    "            # 인덱스: 예측 클래스 <--> 실제 클래스\n",
    "            print(\"{0:2}: {1:6,.2f} <--> {2:6,.2f}\".format(\n",
    "                idx,                            # 데이터 포인트 인덱스\n",
    "                torch.argmax(output).item(),    # 예측한 클래스 (0: 하락, 1: 상승)\n",
    "                target.item()                   # 실제 클래스\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # 현재 시간을 문자열로 변환 (실행 식별자로 사용)\n",
    "    run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    # wandb 설정을 위한 하이퍼파라미터 딕셔너리 생성\n",
    "    config = {\n",
    "        'epochs': args.epochs,                    # 총 학습 에폭 수\n",
    "        'batch_size': args.batch_size,           # 미니배치 크기\n",
    "        'validation_intervals': args.validation_intervals,  # 검증 수행 주기\n",
    "        'learning_rate': args.learning_rate,      # 학습률\n",
    "        'early_stop_patience': args.early_stop_patience,  # 조기 종료 인내 횟수\n",
    "        'early_stop_delta': args.early_stop_delta,  # 조기 종료 임계값\n",
    "    }\n",
    "\n",
    "    # wandb 프로젝트 초기화 (테스트 모드)\n",
    "    project_name = \"lstm_classification_btc_krw_next_open\"\n",
    "    wandb.init(\n",
    "        mode=\"disabled\",                          # 테스트 시에는 wandb 비활성화\n",
    "        project=project_name,                     # 프로젝트 이름\n",
    "        notes=\"btc_krw experiment with lstm\",     # 실험 설명\n",
    "        tags=[\"lstm\", \"regression\", \"btc_krw\"],   # 관련 태그\n",
    "        name=run_time_str,                       # 실행 이름 (시간 기반)\n",
    "        config=config                            # 설정값들\n",
    "    )\n",
    "\n",
    "    # 분류 모델 인스턴스 생성\n",
    "    test_model = get_model()\n",
    "\n",
    "    # 저장된 최신 모델 파일 경로 설정\n",
    "    latest_file_path = os.path.join(\n",
    "        CHECKPOINT_FILE_PATH, f\"{project_name}_checkpoint_2024-12-12_17-13-45.pt\"    # 최신 체크포인트 파일명\n",
    "    )\n",
    "    print(\"MODEL FILE: {0}\".format(latest_file_path))\n",
    "    \n",
    "    # 저장된 모델의 가중치를 CPU에 로드\n",
    "    test_model.load_state_dict(\n",
    "        torch.load(latest_file_path, map_location=torch.device('cpu'))\n",
    "    )\n",
    "\n",
    "    # 테스트 수행\n",
    "    test(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FILE: /home/Deep-Learning-study/_02_homeworks/hw4/checkpoints/lstm_classification_btc_krw_next_open_checkpoint_2024-12-12_17-13-45.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_122797/1596500661.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(latest_file_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST DATA]\n",
      "TEST RESULTS: 50.000%\n",
      " 0:   1.00 <-->   1.00\n",
      " 1:   1.00 <-->   1.00\n",
      " 2:   1.00 <-->   0.00\n",
      " 3:   1.00 <-->   0.00\n",
      " 4:   1.00 <-->   1.00\n",
      " 5:   1.00 <-->   1.00\n",
      " 6:   1.00 <-->   0.00\n",
      " 7:   1.00 <-->   0.00\n",
      " 8:   1.00 <-->   0.00\n",
      " 9:   1.00 <-->   1.00\n",
      "10:   1.00 <-->   0.00\n",
      "11:   1.00 <-->   0.00\n",
      "12:   1.00 <-->   1.00\n",
      "13:   1.00 <-->   1.00\n",
      "14:   1.00 <-->   0.00\n",
      "15:   1.00 <-->   0.00\n",
      "16:   1.00 <-->   0.00\n",
      "17:   1.00 <-->   1.00\n",
      "18:   1.00 <-->   1.00\n",
      "19:   1.00 <-->   0.00\n",
      "20:   1.00 <-->   1.00\n",
      "21:   1.00 <-->   0.00\n",
      "22:   1.00 <-->   1.00\n",
      "23:   1.00 <-->   0.00\n",
      "24:   1.00 <-->   1.00\n",
      "25:   1.00 <-->   1.00\n",
      "26:   1.00 <-->   1.00\n",
      "27:   1.00 <-->   0.00\n",
      "28:   1.00 <-->   1.00\n",
      "29:   1.00 <-->   0.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if 'ipykernel' in sys.modules:  # Jupyter Notebook에서 실행 중인지 확인\n",
    "        # Jupyter에서 실행할 때는 기본값 사용\n",
    "        args = Args()\n",
    "    else:\n",
    "        # 일반 Python 스크립트로 실행할 때는 argparse 사용\n",
    "        parser = get_parser()\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숙제 후기에서도 계속 이어서 이야기 하겠지만 lstm 네트워크를 어느정도 많이 쌓는데는 한계가 있는것 같다. 사실 학습을 하면서 여러 시도를 하면서 차라리 overfitting이 날때 까지 돌려보려고도 하긴 하였는데 overfitting도 나지 않는것을 확인할 수 있었다. 기존에 대비 feature를 추가해도 그렇게 유의미한 변화를 파악할 수 없었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숙제 후기\n",
    "\n",
    "lstm에 대해서 굉장히 많은 모델 아키텍쳐 변경을 통한 학습을 시도해보았지만 그 한계를 맛볼 수 있었던 과제였습니다. iteration을 굉장히 많이 해보기도 하고, laerning rate를 굉장히 작게 만들어 local minima에 빠지는것을 막고자 하였지만 결국 그 마지막은 비슷한 성능을 냄을 알 수 있었습니다.\n",
    "결국 그 구간이 local minima가 아니라 global minima라고 생각이 들었던것 같아, 이것은 lstm 자체의 모델의 해결하지 문제점이 아닌가라는 생각이 많이 드는 과제였습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
